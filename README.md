
<div align="center">
<img src="./assets/bleeding.png" style="width: 30%;height: 30%">
</div>

# Model-Hemorrhage
Model Hemorrhage and the Robustness Limits of Large Language  Models: A Perspective

## 📜 Paper
Our full paper is available on arXiv: [**Model Hemorrhage and the Robustness Limits of Large Language Models**](https://doi.org/10.48550/arXiv.2503.23924) [![Paper](https://img.shields.io/badge/Paper-%F0%9F%8E%93-lightblue?style=flat-square)](https://doi.org/10.48550/arXiv.2503.23924)

## 📬 Contact
If you find any errors or have suggestions, feel free to reach out: **maziyang@whu.edu.cn**

---

## 🌐 About
In our work, we introduce the concept of Model Hemorrhage, a comprehensive framework that investigates how optimization techniques—such as pruning, quantization, and decoding adaptations—can lead to unexpected degradation in performance and stability. Through empirical analysis and theoretical insights, we reveal five key dimensions of fragility in LLMs: architectural redundancy, model compression, training-inference, extension mechanisms, and data-related vulnerabilities. To ground this framework, we present empirical case studies that illuminate key trade-offs—including cardinal sparsity thresholds for pruning, the lossless quantization thresholds, full-size progressive quantization, and horizontal comparisons across different compression strategies and decoding methods.

